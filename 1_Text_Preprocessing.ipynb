{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing\n",
        "Before we can use text data for Machine Learning or Deep Learning, we need to clean and standardize it. Raw text contains punctuation, stopwords, special characters, and inconsistencies that can reduce model performance.\n",
        "\n",
        "### Steps in Text Preprocessing\n",
        "1️⃣ Tokenization – Splitting text into words or sentences\n",
        "\n",
        "2️⃣ Stopword Removal – Removing common words (e.g., \"is\", \"the\", \"and\")\n",
        "\n",
        "3️⃣ Stemming & Lemmatization – Converting words to their root form\n",
        "\n",
        "4️⃣ Text Normalization – Lowercasing, removing special characters, numbers, etc"
      ],
      "metadata": {
        "id": "tN0AtyODKQ2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "Tokenization is the process of splitting text into smaller units (tokens), such as words or sentences.\n",
        "\n",
        "##Types of Tokenization:\n",
        "\n",
        "1)Word Tokenization → Splits text into words\n",
        "\n",
        "2)Sentence Tokenization → Splits text into sentences\n",
        "\n",
        "## Example:\n",
        " Input: \"I love NLP! It's amazing.\"\n",
        "\n",
        " Word Tokenization Output: [\"I\", \"love\", \"NLP\", \"!\", \"It\", \"'s\", \"amazing\", \".\"]\n",
        "\n",
        " Sentence Tokenization Output: [\"I love NLP!\", \"It's amazing.\"]"
      ],
      "metadata": {
        "id": "_FnBQKdyG-zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation"
      ],
      "metadata": {
        "id": "jKLZf0HOHlZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n"
      ],
      "metadata": {
        "id": "7dnF89xXHdfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab') #download tokenizer data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zZAOek2HsTf",
        "outputId": "db7447b9-7e41-4e03-999f-b0ce69c7f5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I love NLP! It's amazing. And I'm excited to learn more about it.\""
      ],
      "metadata": {
        "id": "KhFswevUH4-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens=word_tokenize(text)\n",
        "sentence_tokens=sent_tokenize(text)"
      ],
      "metadata": {
        "id": "F34xJfvOIHVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization : ',word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peyFiKQxIcim",
        "outputId": "1f9c020d-31f4-4e15-8f04-02d02aededad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization :  ['I', 'love', 'NLP', '!', 'It', \"'s\", 'amazing', '.', 'And', 'I', \"'m\", 'excited', 'to', 'learn', 'more', 'about', 'it', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Sentence tokenization : ',sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VANvO9hJAAg",
        "outputId": "ed2ad603-53d9-459d-c88e-bd4a13f7ccce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence tokenization :  ['I love NLP!', \"It's amazing.\", \"And I'm excited to learn more about it.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wVWBFYeJK1TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopword Removal\n",
        "### What are Stopwords?\n",
        "Stopwords are common words like \"is\", \"the\", \"and\", \"in\", which do not add much meaning to the text but increase the size of data. Removing them helps models focus on important words.\n",
        "\n",
        "\n",
        "### Example:\n",
        " Input: \"I love NLP because it is amazing!\"\n",
        "\n",
        " Without Stopwords: [\"love\", \"NLP\", \"amazing\"]"
      ],
      "metadata": {
        "id": "cQX35adKLVeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA-CSSv4LjuU",
        "outputId": "a5e43780-a7fa-447d-d05e-f7d6c0a95777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text='I love NLP because it is amazing!, and I am excited to learn more about it.'"
      ],
      "metadata": {
        "id": "67-ndnPZMF5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=word_tokenize(text)"
      ],
      "metadata": {
        "id": "yEKQ9mpdMQEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' words before stopword removal : ',words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz02pR35MbL_",
        "outputId": "d1dd11bd-3c04-4f65-c926-ef1573b205bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " words before stopword removal :  ['I', 'love', 'NLP', 'because', 'it', 'is', 'amazing', '!', ',', 'and', 'I', 'am', 'excited', 'to', 'learn', 'more', 'about', 'it', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords=set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "YTInV4ijMcP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filterd_words=[ word for word in words if word.lower() not in stopwords]  #removing stopwords"
      ],
      "metadata": {
        "id": "MWQtVlKGM43d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' words after stopword removal : ',filterd_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gBr79y-M6lh",
        "outputId": "22a06c99-d3b7-4baa-8814-c42eb0dd3838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " words after stopword removal :  ['love', 'NLP', 'amazing', '!', ',', 'excited', 'learn', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNPIkl_bNj44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming & Lemmatization\n",
        "Both Stemming and Lemmatization reduce words to their base or root form, but they work differently.\n",
        "\n",
        " ### 1. Stemming\n",
        "Stemming removes suffixes to get the root form of a word. It does not always produce real words but is faster.\n",
        "\n",
        "### Example:\n",
        " \"running\" → \"run\"\n",
        "\n",
        " \"better\" → \"bet\"\n"
      ],
      "metadata": {
        "id": "NXbq7Kd09lcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "nYPi6bAQ_C00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n",
        "words=['running','flies','easily','fairly']\n",
        "stemmed_words=[stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCVbvqVjEWMc",
        "outputId": "40d86dde-eac0-485d-c95a-231a25e535cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fli', 'easili', 'fairli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  2. Lemmatization\n",
        "Lemmatization converts words into their meaningful root form, considering grammar. It produces actual words but is slower than stemming.\n",
        "\n",
        "### Example:\n",
        "\"running\" → \"run\"\n",
        "\n",
        " \"better\" → \"good\""
      ],
      "metadata": {
        "id": "gT7nrwKtEMZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEcLnv97ENJw",
        "outputId": "b9cc59b5-534d-4bd2-c917-9a0502f7315f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmantizer=WordNetLemmatizer()\n",
        "lemmantized_words=[lemmantizer.lemmatize(word) for word in words]\n",
        "print(lemmantized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za-H25bJFFSu",
        "outputId": "ff9bd989-04aa-46d1-f93a-354ffdad2600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'fly', 'easily', 'fairly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iN56FNWCF3YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Normalization\n",
        " In NLP, text can be written in different ways, and normalization ensures consistency.\n",
        "\n",
        "### Common Text Normalization Techniques:\n",
        "\n",
        "Lowercasing – Convert text to lowercase\n",
        "\n",
        "Removing Punctuation – Remove symbols like .,!?\n",
        "\n",
        "Removing Numbers – Remove digits if not needed\n",
        "\n",
        "Expanding Contractions – Convert \"can't\" → \"cannot\"\n",
        "\n",
        "Removing Special Characters – Remove #, @, * etc.\n",
        "\n",
        "Removing Extra Whitespaces – Remove unnecessary spaces"
      ],
      "metadata": {
        "id": "ZZEvdeler7Ae"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iunphIDOsEiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lowercasing"
      ],
      "metadata": {
        "id": "N2iW_CPNsJW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLP is AMAZING!\"\n",
        "lower_text = text.lower()\n",
        "print(lower_text)  # Output: nlp is amazing!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ora7M-12sN3w",
        "outputId": "4e806ddd-0528-4a80-9697-1b44c5f6f33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nlp is amazing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing punctuation"
      ],
      "metadata": {
        "id": "DV63afgQsREe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello! How's NLP?\"\n",
        "clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(clean_text)  # Output: Hello Hows NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gypSn3M5sPfo",
        "outputId": "9473df2d-839c-474a-c29a-7698530bd769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Hows NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Numbers"
      ],
      "metadata": {
        "id": "MuCXCPLJslnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"I have 2 cats and 3 dogs\"\n",
        "clean_text = re.sub(r'\\d+', '', text)\n",
        "print(clean_text)  # Output: I have cats and dogs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmz7qqI5sZQm",
        "outputId": "a8a9ba40-7e2f-4e31-a2d2-3c4f101537c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have  cats and  dogs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expanding Contractions"
      ],
      "metadata": {
        "id": "q3-fOsAEswMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "from contractions import fix\n",
        "\n",
        "text = \"I'll go, but I can't stay.\"\n",
        "expanded_text = fix(text)\n",
        "print(expanded_text)  # Output: I will go, but I cannot stay.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FYLdJHaQsrva",
        "outputId": "f1b31b14-e412-4eda-82be-16030967aec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
            "I will go, but I cannot stay.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Special Characters"
      ],
      "metadata": {
        "id": "sOLm-ZfRtQWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"@NLP is #amazing!!\"\n",
        "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "print(clean_text)  # Output: NLP is amazing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEWx300Hs0hm",
        "outputId": "5bd23099-832f-4ea1-9135-433e08874660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP is amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Extra White Spaces"
      ],
      "metadata": {
        "id": "Y7SJH3fxtkQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" NLP    is     amazing!   \"\n",
        "clean_text = \" \".join(text.split())\n",
        "print(clean_text)  # Output: NLP is amazing!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1alpyAJtb2J",
        "outputId": "eab1dfde-7afb-40f1-cc99-3f1deb9a8ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP is amazing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyDpxm6ttpYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-Of-Speech(POS) Tagging"
      ],
      "metadata": {
        "id": "78_uB8PZuIBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS tagging assigns grammatical labels to words in a sentence. This helps NLP models understand the structure and meaning of text.\n",
        "\n",
        "### Example:\n",
        " Sentence: \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        " POS Tags:\n",
        "\n",
        "The → Determiner (DT)\n",
        "\n",
        "quick → Adjective (JJ)\n",
        "\n",
        "fox → Noun (NN)\n",
        "\n",
        "jumps → Verb (VBZ)\n",
        "\n"
      ],
      "metadata": {
        "id": "wc7Q68PUuTN3"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "# Download the 'averaged_perceptron_tagger_eng' resource specifically\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Get POS tags\n",
        "pos_tags = pos_tag(words)\n",
        "print(pos_tags)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbCET5yDvNlE",
        "outputId": "83228368-4058-469a-8e02-91a0561f6866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTF5y2YruoUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}