{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Representation"
      ],
      "metadata": {
        "id": "X4vnJO1Hkxnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before feeding text into machine learning models, we must convert it into numerical format. This process is called text representation.\n",
        "### Text Representation Techniques\n",
        "1Ô∏è‚É£ Bag of Words (BoW) - (Simple, Count-Based)\n",
        "\n",
        "2Ô∏è‚É£ TF-IDF (Term Frequency-Inverse Document Frequency) - (Weight-Based)\n",
        "\n",
        "3Ô∏è‚É£ N-grams (Unigram, Bigram, Trigram) - (Context-Based)\n",
        "\n",
        "4Ô∏è‚É£ Word Embeddings (Word2Vec, GloVe, FastText) - (Deep Learning-Based)"
      ],
      "metadata": {
        "id": "0cOWRymdlEs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words\n",
        "BoW is a simple method that converts text into a word frequency matrix. It ignores word order but keeps track of word occurrence.\n",
        "\n",
        " How does it work?\n",
        "\n",
        " Tokenize text into words.\n",
        "\n",
        " Count the frequency of each word.\n",
        "\n",
        " Create a matrix where rows = sentences/documents and columns = words."
      ],
      "metadata": {
        "id": "hmFTQl4JlPJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text corpus\n",
        "text = [\n",
        "    \"I love NLP\",\n",
        "    \"NLP is awesome\",\n",
        "    \"Deep learning is useful for NLP\"\n",
        "]\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Convert to array\n",
        "print(X.toarray())\n",
        "\n",
        "# Show the feature names (words)\n",
        "print(vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "cLSlx63zyCxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a8b283-7485-4178-88bc-06d49cf2f1f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 1 1 0]\n",
            " [1 0 0 1 0 0 1 0]\n",
            " [0 1 1 1 1 0 1 1]]\n",
            "['awesome' 'deep' 'for' 'is' 'learning' 'love' 'nlp' 'useful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "aKlKknTzoWIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF is an improvement over Bag of Words (BoW). Instead of just counting words, it assigns weights to words based on their importance in a document.\n",
        "\n",
        " Common words (like \"is\", \"the\") get lower importance.\n",
        "\n",
        " Rare but important words (like \"NLP\", \"Transformer\") get higher importance.\n",
        "\n",
        " Reduces the dominance of frequently occurring but less meaningful words."
      ],
      "metadata": {
        "id": "eRTAiOsjoZ57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text corpus\n",
        "text = [\n",
        "    \"I love NLP\",\n",
        "    \"NLP is amazing\",\n",
        "    \"Deep learning is useful for NLP\"\n",
        "]\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Convert to array\n",
        "print(X.toarray())\n",
        "\n",
        "# Show the feature names (words)\n",
        "print(vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgFc9yY8luaY",
        "outputId": "4725ac31-be22-4a23-dab4-9c46638581b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.861037\n",
            "  0.50854232 0.        ]\n",
            " [0.72033345 0.         0.         0.54783215 0.         0.\n",
            "  0.42544054 0.        ]\n",
            " [0.         0.45050407 0.45050407 0.34261996 0.45050407 0.\n",
            "  0.26607496 0.45050407]]\n",
            "['amazing' 'deep' 'for' 'is' 'learning' 'love' 'nlp' 'useful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-grams\n",
        "N-grams are continuous sequences of N words from a given text. They help capture context and word relationships in NLP.\n",
        "\n",
        "### Types of N-grams:\n",
        "1Ô∏è‚É£ Unigram ‚Üí Single words (e.g., \"I\", \"love\", \"NLP\")\n",
        "\n",
        "2Ô∏è‚É£ Bigram ‚Üí Two-word sequences (e.g., \"I love\", \"love NLP\")\n",
        "\n",
        "3Ô∏è‚É£ Trigram ‚Üí Three-word sequences (e.g., \"I love NLP\")\n",
        "\n",
        "4Ô∏è‚É£ n-gram ‚Üí Any N-word sequence\n",
        "\n",
        "üìå Why Use N-grams?\n",
        "\n",
        "‚úîÔ∏è Captures context beyond individual words\n",
        "\n",
        "‚úîÔ∏è Improves text classification and language modeling\n",
        "\n",
        "‚úîÔ∏è Helps in autocorrect, autocomplete, and speech recognition\n",
        "\n"
      ],
      "metadata": {
        "id": "4OQsuedXrO5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "    \"I love NLP\",\n",
        "    \"NLP is amazing\",\n",
        "    \"Deep learning is useful for NLP\"\n",
        "]\n",
        "\n",
        "# Unigram\n",
        "vectorizer_unigram = CountVectorizer(ngram_range=(1,1))\n",
        "X_unigram = vectorizer_unigram.fit_transform(corpus)\n",
        "print(\"Unigram Feature Names:\", vectorizer_unigram.get_feature_names_out())\n",
        "\n",
        "# Bigram\n",
        "vectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
        "X_bigram = vectorizer_bigram.fit_transform(corpus)\n",
        "print(\"Bigram Feature Names:\", vectorizer_bigram.get_feature_names_out())\n",
        "\n",
        "# Trigram\n",
        "vectorizer_trigram = CountVectorizer(ngram_range=(3,3))\n",
        "X_trigram = vectorizer_trigram.fit_transform(corpus)\n",
        "print(\"Trigram Feature Names:\", vectorizer_trigram.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwYBEyB4okcO",
        "outputId": "541ed699-abca-438b-c962-2107dc26a51f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Feature Names: ['amazing' 'deep' 'for' 'is' 'learning' 'love' 'nlp' 'useful']\n",
            "Bigram Feature Names: ['deep learning' 'for nlp' 'is amazing' 'is useful' 'learning is'\n",
            " 'love nlp' 'nlp is' 'useful for']\n",
            "Trigram Feature Names: ['deep learning is' 'is useful for' 'learning is useful' 'nlp is amazing'\n",
            " 'useful for nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AInD_B9ireYQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings\n",
        "Word embeddings represent words as dense numerical vectors in a high-dimensional space, capturing semantic relationships between words. Unlike one-hot encoding, embeddings preserve meaning and context.\n",
        "\n",
        "### Why Use Word Embeddings\n",
        " Capture semantic similarity (e.g., \"king\" and \"queen\" are close)\n",
        "\n",
        " Improve text classification and sentiment analysis\n",
        "\n",
        " Work well for large vocabularies"
      ],
      "metadata": {
        "id": "GcPc3MyIBhmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of Word Embeddings\n",
        "1Ô∏è‚É£ Word2Vec ‚Äì Predicts words based on context (CBOW, Skip-gram)\n",
        "\n",
        "2Ô∏è‚É£ GloVe ‚Äì Uses co-occurrence matrix for learning word relationships\n",
        "\n",
        "3Ô∏è‚É£ FastText ‚Äì Extends Word2Vec by considering subwords (useful for misspellings and rare words)"
      ],
      "metadata": {
        "id": "5k-KNWhcB6Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ePRqkZ6tCDwt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}