{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling\n",
        " Topic Modeling is an unsupervised learning technique that discovers hidden topics in a collection of text documents.\n",
        "\n",
        " It helps in document classification, information retrieval, and summarization.\n",
        "### Example:\n",
        "A news website can use topic modeling to group articles into topics like \"Sports,\" \"Politics,\" \"Technology,\" etc.\n",
        "\n",
        " ### Steps for Topic Modeling\n",
        " 1. Load and preprocess text data\n",
        "\n",
        " 2. Convert text into numerical format (BoW, TF-IDF)\n",
        "\n",
        " 3. Apply Topic Modeling algorithms:\n",
        "  -  Latent Semantic Analysis (LSA)\n",
        "  - Latent Dirichlet Allocation (LDA)\n",
        "  - Non-negative Matrix Factorization (NMF)"
      ],
      "metadata": {
        "id": "_SpRR1CLAIeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Load and Preprocess Text Data\n",
        "We will use the 20 Newsgroups dataset (a collection of news articles)."
      ],
      "metadata": {
        "id": "81fzG-H1BEX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load dataset\n",
        "categories = ['rec.sport.baseball', 'rec.sport.hockey', 'sci.space', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Preprocess: Convert text to TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=2000)\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "\n",
        "print(f\"Dataset Size: {X.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0QiA7KFAxtR",
        "outputId": "eb6301e3-3e64-4f9b-f8c4-50a2418a4ade"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size: (3953, 2000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Semantic Analysis (LSA)\n",
        " LSA uses Singular Value Decomposition (SVD) to reduce dimensionality and find topics."
      ],
      "metadata": {
        "id": "0i3NMIM8BpoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Apply LSA\n",
        "lsa = TruncatedSVD(n_components=5, random_state=42)\n",
        "lsa_topics = lsa.fit_transform(X)\n",
        "\n",
        "# Display top words in each topic\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i, topic in enumerate(lsa.components_):\n",
        "    top_words = [terms[i] for i in topic.argsort()[-10:]]\n",
        "    print(f\"Topic {i+1}: {', '.join(top_words)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjSvTuzpBoPY",
        "outputId": "7a9c166b-61aa-40db-ed7d-c38f143142d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: time, good, team, know, year, just, think, like, don, game\n",
            "Topic 2: espn, baseball, players, play, season, hockey, year, games, team, game\n",
            "Topic 3: think, cost, mission, earth, moon, orbit, launch, shuttle, nasa, space\n",
            "Topic 4: 18, period, space, 16, 14, 13, 15, 12, 11, 10\n",
            "Topic 5: coverage, nasa, thanks, night, baseball, games, hockey, space, espn, game\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Dirichlet Allocation (LDA)\n",
        " LDA assumes that each document contains a mixture of multiple topics, and it assigns probabilities to words for each topic."
      ],
      "metadata": {
        "id": "4LGySljjCAdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Apply LDA\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda_topics = lda.fit_transform(X)\n",
        "\n",
        "# Display top words in each topic\n",
        "for i, topic in enumerate(lda.components_):\n",
        "    top_words = [terms[i] for i in topic.argsort()[-10:]]\n",
        "    print(f\"Topic {i+1}: {', '.join(top_words)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6qkj6I7Bvjb",
        "outputId": "f4305a47-b018-4465-f109-f598c67d2265"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: just, like, 10, moon, launch, orbit, earth, shuttle, nasa, space\n",
            "Topic 2: letter, groups, split, rights, just, roger, david, writes, rec, newsgroup\n",
            "Topic 3: good, hockey, like, just, don, think, games, year, team, game\n",
            "Topic 4: sas, appreciated, john, maine, let, gm, traded, edu, captain, jewish\n",
            "Topic 5: software, program, mail, does, file, files, know, image, graphics, thanks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-negative Matrix Factorization (NMF)\n",
        " NMF factorizes the text matrix into parts-based representations to extract topics."
      ],
      "metadata": {
        "id": "9vakJ2DwCRNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Apply NMF\n",
        "nmf = NMF(n_components=5, random_state=42)\n",
        "nmf_topics = nmf.fit_transform(X)\n",
        "\n",
        "# Display top words in each topic\n",
        "for i, topic in enumerate(nmf.components_):\n",
        "    top_words = [terms[i] for i in topic.argsort()[-10:]]\n",
        "    print(f\"Topic {i+1}: {', '.join(top_words)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvmnh1iqB9Sw",
        "outputId": "b1c6c2cd-e8e4-424c-eed5-dd464ecf2052"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: better, time, players, like, good, just, team, year, don, think\n",
            "Topic 2: ftp, program, format, does, file, know, files, image, graphics, thanks\n",
            "Topic 3: cost, station, mission, moon, earth, orbit, launch, shuttle, nasa, space\n",
            "Topic 4: 18, 20, period, 16, 14, 13, 15, 12, 11, 10\n",
            "Topic 5: coverage, series, fans, pens, night, baseball, hockey, espn, games, game\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNKJsy0cCMbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LSA uses SVD for dimensionality reduction.\n",
        "\n",
        "* LDA assumes documents have a mix of multiple topics.\n",
        "\n",
        "* NMF factorizes data into parts-based topics."
      ],
      "metadata": {
        "id": "nWLMHE_NCYoZ"
      }
    }
  ]
}